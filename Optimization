import torch
import torch.nn as nn
import numpy as np


# goal: 7 by 7 parameter "weights" need to be adjusted to minimize loss defined by the square of the trace of the weight matrix


#sigmas

sigma0 = torch.tensor([[1., 0.],
       [0., 1.]], dtype=torch.cfloat)

sigmax = torch.tensor([[0., 1.],
       [1., 0.]], dtype=torch.cfloat)

sigmay = torch.tensor([[0., -1j],
       [1j, 0.]], dtype=torch.cfloat)

sigmaz = torch.tensor([[1, 0.],
       [0., -1]], dtype=torch.cfloat)

I = sigma0

ansatz = torch.randn(4, 4, requires_grad=True, dtype=torch.cfloat)
#print(ansatz)
with torch.no_grad():
    X_input = ansatz + ansatz.conj().T
X_input.requires_grad_()
print("Here is the random ansatz plus its conjugate transpose:")
print(X_input)
print("\n")
H_F = torch.randn(X_input.shape)
H_I = torch.randn(X_input.shape)
l = 2

def Htheta(theta):
    return -np.cos(theta)*(torch.kron(sigmaz,sigma0)+torch.kron(sigma0,sigmaz)) -np.sin(theta)*(torch.kron(sigmax,sigma0)+torch.kron(sigma0,sigmax)) -3*torch.kron(sigmaz,sigmaz)
    
def dHtheta(theta):
    return np.sin(theta)*(torch.kron(sigmaz,sigma0)+torch.kron(sigma0,sigmaz)) -np.cos(theta)*(torch.kron(sigmax,sigma0)+torch.kron(sigma0,sigmax))

H = Htheta(np.pi/2)
dtH = dHtheta(np.pi/2)

def G(H_F, H_I, l, X):
    return H_F-H_I+1j*(X@(l*H_F+(1-l)*H_I)-(l*H_F+(1-l)*H_I)@X)

def G_test(H, dtH, X):
    return dtH-(X@H-H@X)

def Cost(H_F, H_I, l, X):
    X=0.5*(X+X.conj().T)
    return torch.trace(G(H_F, H_I, l, X)@G(H_F, H_I, l, X))

def Cost_test(H, X):
    X=X+X.conj().T
    return torch.trace(G_test(H, dtH, X)@G_test(H, dtH, X))

#print(Cost(H_F, H_I, l, X_curr))
#print(X_input)

def minimize(X_input):
    X_curr = X_input
    opt = torch.optim.Adam([X_curr], lr=0.01)
    for _ in range(10000):
        loss = abs(Cost(H_F, H_I, l, X_curr))
        opt.zero_grad()
        loss.backward()
        opt.step()
    X_output = X_curr
    return X_output

def minimize_test(X_input):
    X_curr = X_input
    opt = torch.optim.Adam([X_curr], lr=0.001)
    for _ in range(1000):
        loss = Cost_test(H, X_curr)
        print(Cost_test(H, X_curr))
        loss = abs(loss)
        opt.zero_grad()
        loss.backward()
        opt.step()
    X_output = X_curr
    return X_output


#print("Initial cost: "+ str(abs(Cost(H_F, H_I, l, X_input))))
#X_output = minimize(X_input)
#print("Final cost: "+ str(abs(Cost(H_F, H_I, l, X_output))))

print("Initial cost: "+ str(abs(Cost_test(H, X_input))))
X_output = minimize_test(X_input)
print("Final cost: "+ str(abs(Cost_test(H, X_output))))

print("Here is the final X: ")
print(X_output)
